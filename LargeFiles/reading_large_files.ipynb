{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import glob\n",
    "import timeit\n",
    "from astropy.table import Table\n",
    "import dask.dataframe as dd\n",
    "import threading\n",
    "import os\n",
    "# use the env ../speedPy.yml\n",
    "def summary(name,timeit_results):\n",
    "    r=np.array(timeit_results)\n",
    "    print(f'{name} {r.size} runs: \\n Mean {r.mean()} \\n Standard deviation {r.std()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading many large files\n",
    "Often when searching through large catalogs the data is divided into many large files. This can be slow to read into memory. This is normally avoided by having a database for the data and searching it that way. This is not always the case and can be difficult to query if you don't ordinarily use databases. I'll use the example large csv files here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_large_files=glob.glob('/Users/danielwarshofsky/Desktop/everything/zenodo/all_preds/*.csv')\n",
    "\n",
    "def read_one_file(path): # seems weird to define this but will be useful for later optimizations\n",
    "    df=pd.read_csv(path)\n",
    "    return df\n",
    "\n",
    "def read_all_files(paths):# just loop and concat the other function\n",
    "    df=pd.DataFrame()\n",
    "    for path in paths:\n",
    "        temp=read_one_file(path)\n",
    "        df=pd.concat([df,temp]).reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean 3 file read time : 18.542 sec. Min: 17.557749032974243 Max: 21.573328256607056\n"
     ]
    }
   ],
   "source": [
    "#doing all of the files will take too long. Lets just try 3 files at a time\n",
    "bad_loop_times=np.arange(0,10).astype(float)\n",
    "for i in range(bad_loop_times.shape[0]):\n",
    "    ts=time.time()\n",
    "    a=read_all_files(list_of_large_files[0:3])\n",
    "    te=time.time()\n",
    "    bad_loop_times[i]=te-ts\n",
    "print(f'Mean 3 file read time : {bad_loop_times.mean():.3f} sec. Min: {min(bad_loop_times)} Max: {max(bad_loop_times)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with the minimum time this would be too long if you need to read 100s of files.\n",
    "\n",
    "now lets try with threading (mostly from https://medium.com/codex/reading-files-fast-with-multi-threading-in-python-ff079f40fe56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_threaded_file_reader(file_paths):\n",
    "    threads = []\n",
    "    results = {}\n",
    "\n",
    "    # Define the worker function\n",
    "    def read_file_thread(file_path):\n",
    "        result = read_one_file(file_path)\n",
    "        results[file_path] = result\n",
    "\n",
    "    # Create and start threads\n",
    "    for file_path in file_paths:\n",
    "        thread = threading.Thread(target=read_file_thread, args=(file_path,))\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "\n",
    "    # Wait for all threads to finish\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    # concat things\n",
    "    df=pd.concat([results[key] for key in results.keys() ]).reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean 3 files read time : 11.094 sec. Min: 10.58876895904541 Max: 11.500808954238892\n",
      "Multi thread on average has a  40.165% speed increase over bad loop\n"
     ]
    }
   ],
   "source": [
    "multi_thread_times=np.arange(0,10).astype(float)\n",
    "for i in range(multi_thread_times.shape[0]):\n",
    "    ts=time.time()\n",
    "    a=multi_threaded_file_reader(list_of_large_files[0:3])\n",
    "    te=time.time()\n",
    "    multi_thread_times[i]=te-ts\n",
    "print(f'Mean 3 files read time : {multi_thread_times.mean():.3f} sec. Min: {min(multi_thread_times)} Max: {max(multi_thread_times)}')\n",
    "print(f'Multi thread on average has a  {100*(bad_loop_times.mean()-multi_thread_times.mean())/bad_loop_times.mean():.3f}% speed increase over bad loop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about Dask?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean 3 files read time : 6.134 sec. Min: 5.608299970626831 Max: 7.671574115753174\n",
      "Dask on average has a  66.919% speed increase over bad loop\n"
     ]
    }
   ],
   "source": [
    "dask_times=np.arange(0,10).astype(float)\n",
    "for i in range(dask_times.shape[0]):\n",
    "    ts=time.time()\n",
    "    df=dd.read_csv(list_of_large_files[0:3])\n",
    "    df.compute()\n",
    "    te=time.time()\n",
    "    dask_times[i]=te-ts\n",
    "print(f'Mean 3 files read time : {dask_times.mean():.3f} sec. Min: {min(dask_times)} Max: {max(dask_times)}')\n",
    "print(f'Dask on average has a  {100*(bad_loop_times.mean()-dask_times.mean())/bad_loop_times.mean():.3f}% speed increase over bad loop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask is much faster than the rest; what is it doing different? Dask efficiently parallelizes the reading by managing many smaller dataframes called partitions. Operations can be preformed on all of the partitions in parallel. However when you call .compute() everything is concatanated together which can take some time.  Lets see how doing an operation on the data before or after the .compute() affects the timeing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First: 5.070731568336487  Last: 6.331090259552002\n"
     ]
    }
   ],
   "source": [
    "var='vnv_dnn > .9 & vnv_xgb > .9'\n",
    "per='pnp_dnn > .9 & pnp_xgb > .9'\n",
    "eclipse='e_dnn > .9 & e_xgb > .9'\n",
    "non_acc='bis_dnn > .7 & bis_xgb > .7'\n",
    "q=f'{var} & {per} & {eclipse} & {non_acc}' #some simple filter\n",
    "\n",
    "q_last_times=np.arange(0,10).astype(float)\n",
    "for i in range(q_last_times.shape[0]):\n",
    "    df=dd.read_csv(list_of_large_files[0:3])\n",
    "    ts=time.time()\n",
    "    df.compute()\n",
    "    df=df.query(q)\n",
    "    te=time.time()\n",
    "    q_last_times[i]=te-ts\n",
    "\n",
    "q_first_times=np.arange(0,10).astype(float)\n",
    "for i in range(q_first_times.shape[0]):\n",
    "    df=dd.read_csv(list_of_large_files[0:3])\n",
    "    ts=time.time()\n",
    "    df=df.query(q)\n",
    "    df.compute()\n",
    "    te=time.time()\n",
    "    q_first_times[i]=te-ts\n",
    "\n",
    "print(f'First: {q_first_times.mean()}  Last: {q_last_times.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing the operation first is a better time wise and is much better memory wise. This is how you should do this in the future!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File format\n",
    "This whole time we have assumed that the data is in csv format. How do other file formats compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a large array\n",
    "waste_of_memory=[np.random.uniform(size=int(1e5)) for i in range(10)]# 1 mill floats\n",
    "col_names=[str(i) for i in range(10)]\n",
    "waste_table=Table(waste_of_memory,names=col_names)\n",
    "del waste_of_memory\n",
    "waste_table.write('waste.parquet',format='parquet',overwrite=True)\n",
    "waste_table.write('waste.hdf5',format='hdf5',overwrite=True)\n",
    "waste_table.write('waste.votable',format='votable',overwrite=True)\n",
    "waste_table.write('waste.csv',format='csv',overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets look at the memory footprint of the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdf5 format is 8.004095999999999 MB\n",
      "csv format is 19.268774999999998 MB\n",
      "parquet format is 10.132379 MB\n",
      "votable format is 36.369704 MB\n"
     ]
    }
   ],
   "source": [
    "files=glob.glob('waste.*')\n",
    "for file in files:\n",
    "    print(f'{file.split(\".\")[-1]} format is {os.path.getsize(file)*1e-6} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already we can see that parquet and hdf5 files have a smaller footprint in memory. on to the speed tests!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdf5 read time 10 runs: \n",
      " Mean 0.00900921680040483 \n",
      " Standard deviation 0.0006776081524732835\n",
      "csv read time 10 runs: \n",
      " Mean 0.3706228706996626 \n",
      " Standard deviation 0.007344938178109973\n",
      "parquet read time 10 runs: \n",
      " Mean 0.011296174999733921 \n",
      " Standard deviation 0.008198364424538375\n",
      "votable read time 10 runs: \n",
      " Mean 3.143551466600184 \n",
      " Standard deviation 0.020637295549368342\n"
     ]
    }
   ],
   "source": [
    "for file in files:\n",
    "    fmat=file.split(\".\")[-1]\n",
    "    t=timeit.Timer(f'Table.read(file)',globals=globals()).repeat(10,1)\n",
    "    summary(f'{fmat} read time',t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that hdf5 and parquet have smaller memory footprints and can be read much faster! Consider using them in the future!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.4 ('speedPy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8546854d9a4d22cf7d2e58b1f2f018da73cc257224ade4622f99aada43e3d768"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
